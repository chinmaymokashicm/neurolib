"""
Aggregate stats generated by BIDS processes on individual scans/files on a dataset level.
"""
from ..scan.region import ParcellationAtlas

from pathlib import Path, PosixPath
from typing import Optional
import itertools

from bids import BIDSLayout
from bids.layout import parse_file_entities
from bids.layout.writing import build_path
import pandas as pd
from rich.progress import track

def aggregate_individual_stats(bids_root: str | PosixPath, bids_filters: dict, pipeline_name: str, extension: str = ".csv", overwrite: bool = False) -> Optional[pd.DataFrame]:
    """
    Aggregate stats generated by BIDS processes on individual scans/files on a dataset level.
    """
    aggregated_stats_filename: str = f"aggregated_stats_{bids_filters['desc']}.csv" if "desc" in bids_filters else "aggregated_stats.csv"
    aggregated_stats_path: PosixPath = Path(bids_root) / "derivatives" / pipeline_name / aggregated_stats_filename
    if not overwrite and aggregated_stats_path.exists():
        df: pd.DataFrame = pd.read_csv(aggregated_stats_path, dtype=str)
        print(f"Overwrite is set to False and {aggregated_stats_path} already exists. Returning the existing aggregated stats.")
        return df
    bids_root: str = str(bids_root)
    layout = BIDSLayout(bids_root, derivatives=True)
    files: list[str] = layout.get(return_type="file", extension=extension, **bids_filters)
    if len(files) == 0:
        print(f"No files found for {bids_filters} in {bids_root} pipeline {pipeline_name} with extension {extension} to aggregate stats.")
        return None
    all_records: list[dict] = []
    for file in track(files, description=f"Aggregating stats on {len(files)} files"):
        entities: dict = parse_file_entities(file)
        subject, session, run = entities.get("subject"), entities.get("session", None), entities.get("run", None)
        df_stats: pd.DataFrame = pd.read_csv(file, dtype=str)
        records: list[dict] = df_stats.to_dict(orient="records")
        for record in records:
            record["subject"] = subject
            record["session"] = session
            record["run"] = run
            record["path"] = file
            all_records.append(record)
    # Save aggregated stats to a CSV file
    df = pd.DataFrame(all_records)
    new_column_order: list[str] = ["subject", "session", "run"] + [col for col in df.columns if col not in ["subject", "session", "run", "path"]] + ["path"]
    df = df[new_column_order]
    if not overwrite and not aggregated_stats_path.exists():
        aggregated_stats_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(aggregated_stats_path, index=False)
    return df


def aggregate_all_stats_by_region(bids_root: str | PosixPath, region_name: str, pipeline_name: str, extension: str = ".csv", overwrite: bool = False) -> Optional[pd.DataFrame]:
    """
    Aggregate all stats generated by a particular region-based computation on individual scans/files on a dataset level.
    """
    aggregated_stats_filename: str = f"aggregated_stats_{region_name}.csv"
    aggregated_stats_path: PosixPath = Path(bids_root) / "derivatives" / pipeline_name / aggregated_stats_filename
    if not overwrite and aggregated_stats_path.exists():
        df: pd.DataFrame = pd.read_csv(aggregated_stats_path, dtype=str)
        print(f"Overwrite is set to False and {aggregated_stats_path} already exists. Returning the existing aggregated stats.")
        return df
    bids_root: str = str(bids_root)
    layout = BIDSLayout(bids_root, derivatives=True)
    region_stats_files_non_aggregated: list[str] = [
        filepath for filepath in layout.get(return_type="file", suffix="stats", extension=extension) if region_name in parse_file_entities(filepath).get("desc", "")
        ]
    if len(region_stats_files_non_aggregated) == 0:
        print(f"No files found for {region_name} in {bids_root} pipeline {pipeline_name} with extension {extension} to aggregate stats.")
        return None
    
    hashed_records: dict[str, dict] = {} #Hash records by subject, session, run, Region
    for file in track(region_stats_files_non_aggregated, description=f"Aggregating stats on {len(region_stats_files_non_aggregated)} files"):
        entities: dict = parse_file_entities(file)
        subject, session, run = entities.get("subject"), entities.get("session", None), entities.get("run", None)
        df_stats: pd.DataFrame = pd.read_csv(file, dtype=str)
        for row in df_stats.iterrows():
            record: dict = row[1].to_dict()
            region: str = record.get("Region")
            hash_key: str = f"{subject}+{session}+{run}+{region}"
            if hash_key not in hashed_records:
                hashed_records[hash_key] = {}
            # Get the remaining columns
            for col in df_stats.columns:
                if col not in ["Region"]:
                    hashed_records[f"{subject}+{session}+{run}+{region}"][col] = record[col]
            hashed_records[hash_key]["path"] = file
    
    # Convert hashed records to a list of dictionaries
    all_records: list[dict] = []
    for key, record in hashed_records.items():
        subject, session, run, region = key.split("+")
        record["subject"] = subject
        record["session"] = session
        record["run"] = run
        record["Region"] = region
        all_records.append(record)
        
    # Save aggregated stats to a CSV file
    df = pd.DataFrame(all_records)
    new_column_order: list[str] = ["subject", "session", "run", "Region"] + [col for col in df.columns if col not in ["subject", "session", "run", "path", "Region"]] + ["path"]
    df = df[new_column_order]
    if (not overwrite and not aggregated_stats_path.exists()) or overwrite:
        aggregated_stats_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(aggregated_stats_path, index=False)
        print(f"Aggregated stats saved to {aggregated_stats_path}")
    return df